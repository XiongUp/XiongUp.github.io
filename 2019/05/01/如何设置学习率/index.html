<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="如何确定一个相对合适的学习率"><meta name="keywords" content><meta name="author" content="XiongXiaoJun"><meta name="copyright" content="XiongXiaoJun"><title>如何确定一个相对合适的学习率 | xJun</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#为何要寻找合适的学习率lr"><span class="toc-number">1.</span> <span class="toc-text">为何要寻找合适的学习率lr</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何寻找？"><span class="toc-number">2.</span> <span class="toc-text">如何寻找？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实践"><span class="toc-number">3.</span> <span class="toc-text">实践</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">XiongXiaoJun</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">2</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1582272735280&amp;di=c4e8528d32b99672ceb69cdb98be8d52&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.cnr.cn%2Ftj%2Fjrtj%2F20200220%2FW020200220430407281064.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">xJun</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span></div><div id="post-info"><div id="post-title">如何确定一个相对合适的学习率</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-01</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="为何要寻找合适的学习率lr">为何要寻找合适的学习率lr</h2>
<ul>
<li>经过了大量炼丹的同学都知道，超参数是一个非常玄乎的东西，比如batch size，学习率等，论文中设定的超参数一般都是靠经验决定的。但是超参数往往又特别重要，比如学习率，如果设置了一个太大的学习率，那么loss就爆了，设置的学习率太小，需要等待的时间就特别长，那么我们是否有一个相对合理的办法来决定我们的初始学习率呢？<br>
<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200301014342.png" width="65%"></li>
</ul>
<h2 id="如何寻找？">如何寻找？</h2>
<ul>
<li>文章 “How Do You Find A Good Learning Rate” 提供了一个相对可靠的寻找方法。即在训练的过程中从小到大不断增加学习率，并记录每一轮学习率对应的loss信息，从而绘制出一张loss、学习率关系曲线图。<br>
<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/1900456b9cd6de1946e0d9a250f333da_1556722410978x5gf2f93.png" alt="Loss、Lr曲线图"><br>
通过这张图，我们可以大致确定学习率的范围。</li>
</ul>
<h2 id="实践">实践</h2>
<ul>
<li>
<p>光说不练假把式，这次，我们使用Pytorch(1.1版本)用于图像识别，图像数据CIFAR10。实践代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">   [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                       download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">8</span>,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                      download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">       super(Net, self).__init__()</span><br><span class="line">       self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">18</span>, <span class="number">5</span>)</span><br><span class="line">       self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">       self.conv2 = nn.Conv2d(<span class="number">18</span>, <span class="number">32</span>, <span class="number">5</span>)</span><br><span class="line">       self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">       self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">       self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">       x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">       x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">       x = x.view(<span class="number">-1</span>, <span class="number">32</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">       x = F.relu(self.fc1(x))</span><br><span class="line">       x = F.relu(self.fc2(x))</span><br><span class="line">       x = self.fc3(x)</span><br><span class="line">       <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lr</span><span class="params">(init_value = <span class="number">1e-5</span>, final_value=<span class="number">1e-1</span>, beta = <span class="number">0.98</span>)</span>:</span></span><br><span class="line">    num = len(trainloader)<span class="number">-1</span></span><br><span class="line">    mult = (final_value / init_value) ** (<span class="number">1</span>/num)</span><br><span class="line">    lr = init_value</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line">    avg_loss = <span class="number">0.</span></span><br><span class="line">    best_loss = <span class="number">0.</span></span><br><span class="line">    batch_num = <span class="number">0</span></span><br><span class="line">    losses = []</span><br><span class="line">    log_lrs = []</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> trainloader:</span><br><span class="line">        batch_num += <span class="number">1</span></span><br><span class="line">        <span class="comment">#As before, get the loss for this mini-batch of inputs/outputs</span></span><br><span class="line">        inputs,labels = data</span><br><span class="line">        inputs, labels = inputs.to(device),labels.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment">#Compute the smoothed loss</span></span><br><span class="line">        avg_loss = beta * avg_loss + (<span class="number">1</span>-beta) *loss.data.item()</span><br><span class="line">        smoothed_loss = avg_loss / (<span class="number">1</span> - beta**batch_num)</span><br><span class="line">        <span class="comment">#Stop if the loss is exploding</span></span><br><span class="line">        <span class="keyword">if</span> batch_num &gt; <span class="number">1</span> <span class="keyword">and</span> smoothed_loss &gt; <span class="number">4</span> * best_loss:</span><br><span class="line">            <span class="keyword">return</span> log_lrs, losses</span><br><span class="line">        <span class="comment">#Record the best loss</span></span><br><span class="line">        <span class="keyword">if</span> smoothed_loss &lt; best_loss <span class="keyword">or</span> batch_num==<span class="number">1</span>:</span><br><span class="line">            best_loss = smoothed_loss</span><br><span class="line">        <span class="comment">#Store the values</span></span><br><span class="line">        losses.append(smoothed_loss)</span><br><span class="line">        log_lrs.append(math.log10(lr))</span><br><span class="line">        print(<span class="string">"losses"</span>,smoothed_loss,<span class="string">"  : log_lrs"</span>,math.log10(lr))</span><br><span class="line">        <span class="comment">#Do the SGD step</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment">#Update the lr for the next step</span></span><br><span class="line">        lr *= mult</span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line">    <span class="keyword">return</span> log_lrs, losses</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    net = Net().to(device)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.Adam(net.parameters(), lr=<span class="number">6.30957e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -------------寻找学习率------------------</span></span><br><span class="line">    log_lrs,losses = find_lr()</span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.plot(np.array(log_lrs), np.array(losses), <span class="string">'r-'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="string">''' --- 注释部分为训练部分 找到学习率后这段取消注释----</span></span><br><span class="line"><span class="string">    for epoch in range(2):  # 多批次循环</span></span><br><span class="line"><span class="string">        running_loss = 0.0</span></span><br><span class="line"><span class="string">        for i, data in enumerate(trainloader, 0):</span></span><br><span class="line"><span class="string">            # 获取输入</span></span><br><span class="line"><span class="string">            inputs, labels = data</span></span><br><span class="line"><span class="string">            inputs, labels = inputs.to(device),labels.to(device)</span></span><br><span class="line"><span class="string">            # 梯度置0</span></span><br><span class="line"><span class="string">            optimizer.zero_grad()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            # 正向传播，反向传播，优化</span></span><br><span class="line"><span class="string">            outputs = net(inputs)</span></span><br><span class="line"><span class="string">            loss = criterion(outputs, labels)</span></span><br><span class="line"><span class="string">            loss.backward()</span></span><br><span class="line"><span class="string">            optimizer.step()</span></span><br><span class="line"><span class="string">            # 打印状态信息</span></span><br><span class="line"><span class="string">            running_loss += loss.item()</span></span><br><span class="line"><span class="string">            if i % 100 == 99:    # 每2000批次打印一次</span></span><br><span class="line"><span class="string">                print('[%d, %5d] loss: %.3f' %</span></span><br><span class="line"><span class="string">                      (epoch + 1, i + 1, running_loss / 100))</span></span><br><span class="line"><span class="string">                running_loss = 0.0</span></span><br><span class="line"><span class="string">    print('Finished Training')</span></span><br><span class="line"><span class="string">    dataiter = iter(testloader)</span></span><br><span class="line"><span class="string">    images, labels = dataiter.next()</span></span><br><span class="line"><span class="string">    # 显示图片</span></span><br><span class="line"><span class="string">    imshow(torchvision.utils.make_grid(images))</span></span><br><span class="line"><span class="string">    images, labels = images.to(device), labels.to(device)</span></span><br><span class="line"><span class="string">    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))</span></span><br><span class="line"><span class="string">    outputs = net(images)</span></span><br><span class="line"><span class="string">    _, predicted = torch.max(outputs, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]</span></span><br><span class="line"><span class="string">                                  for j in range(4)))</span></span><br><span class="line"><span class="string">    correct = 0</span></span><br><span class="line"><span class="string">    total = 0</span></span><br><span class="line"><span class="string">    with torch.no_grad():</span></span><br><span class="line"><span class="string">        for data in testloader:</span></span><br><span class="line"><span class="string">            images, labels = data</span></span><br><span class="line"><span class="string">            images, labels = images.to(device),labels.to(device)</span></span><br><span class="line"><span class="string">            outputs = net(images)</span></span><br><span class="line"><span class="string">            _, predicted = torch.max(outputs.data, 1)</span></span><br><span class="line"><span class="string">            total += labels.size(0)</span></span><br><span class="line"><span class="string">            correct += (predicted == labels).sum().item()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print('Accuracy of the net![paste image](http://pqijd5ur8.bkt.clouddn.com/1556723319786h9drvq7w.png)work on the 10000 test images: %d %%' % (</span></span><br><span class="line"><span class="string">            100 * correct / total))</span></span><br><span class="line"><span class="string">    # '''</span></span><br></pre></td></tr></table></figure>
<p>运行代码，我们可以得到这么一张图片：<br>
<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/f8d3810d65d0060a82324099184da3b1_1556723354833hk6ypeif.png" alt="Loss、Lr关系曲线图"><br>
从图像上来看，合适的学习率大致在1e-4.5、1e-3之间，采几个点简单测试（仅训练2轮），结果如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">学习率</th>
<th style="text-align:center">最终Loss</th>
<th style="text-align:center">测试集准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$10^{-4.5}$</td>
<td style="text-align:center">1.41</td>
<td style="text-align:center">48%</td>
</tr>
<tr>
<td style="text-align:center">$10^{-4}$</td>
<td style="text-align:center">1.29</td>
<td style="text-align:center">56%</td>
</tr>
<tr>
<td style="text-align:center">$10^{-3}$</td>
<td style="text-align:center">1.193</td>
<td style="text-align:center">59%</td>
</tr>
<tr>
<td style="text-align:center">$10^{-3.2}$</td>
<td style="text-align:center">1.016</td>
<td style="text-align:center">62%</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>当$lr = 10^{-3.2}$时，训练效果看上去还不错，但是随着训练的次数不断增加，loss不断降低，之前的学习率对于后面的训练来讲可能过大，从而导致Loss不断震荡而迟迟无法达到预期。本次进行两组实验，一组$lr=10^{-3.2}$，一组采用动态的$lr=10^{((1.4673913* loss)^{(1/3)}-4.5)}$。动态lr的指数与loss的关系如下图所示【Loss下降，lr 10的指数随着下降】：<br>
<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221134650.png" alt><br>
该图主要依据我们之前获得的Loss、Lr关系曲线图确定</p>
</li>
<li>
<p>测试结果如下：<br>
动态Lr前期效果与Lr=10^-3.2相当，均可使得Loss迅速下降，但Loss降至0.25后，此时10^-3.2的学习率相对较大，使得Loss不断震荡难以下降，而动态Lr下Loss则表现相对平稳并持续下降。<br>
<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/8aea6d2591ac377ab2a8d55bb06de7c8_15567821029676mc8ef0b.png" alt></p>
</li>
</ul>
<p><a href="https://blog.csdn.net/briblue/article/details/84325722" target="_blank" rel="noopener">https://blog.csdn.net/briblue/article/details/84325722</a><br>
<a href="https://blog.csdn.net/tonydz0523/article/details/79073146" target="_blank" rel="noopener">https://blog.csdn.net/tonydz0523/article/details/79073146</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">XiongXiaoJun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://xiongup.github.io/2019/05/01/如何设置学习率/">https://xiongup.github.io/2019/05/01/如何设置学习率/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://xiongup.github.io">xJun</a>！</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/02/21/本科毕业设计/"><i class="fa fa-chevron-left">  </i><span>毕业设计简介</span></a></div><div class="next-post pull-right"><a href="/2019/04/26/深度学习环境GPU-Cuda、cudnn下载/"><span>深度学习环境GPU-Cuda、cudnn下载</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1582272735280&amp;di=c4e8528d32b99672ceb69cdb98be8d52&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.cnr.cn%2Ftj%2Fjrtj%2F20200220%2FW020200220430407281064.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By XiongXiaoJun</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>