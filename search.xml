<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>npm install 报错</title>
      <link href="/2020/02/29/npm-install-%E6%8A%A5%E9%94%99/"/>
      <url>/2020/02/29/npm-install-%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<h1>npm install 报错</h1><ul><li><p>Failed at the XXX install script</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">npm ERR! code ELIFECYCLE</span><br><span class="line">npm ERR! errno 1</span><br><span class="line">npm ERR! core-js@2.6.11 postinstall: `node -e &quot;try&#123;require(&apos;./postinstall&apos;)&#125;catch(e)&#123;&#125;&quot;`</span><br><span class="line">npm ERR! Exit status 1</span><br><span class="line">npm ERR!</span><br><span class="line">npm ERR! Failed at the core-js@2.6.11 postinstall script.</span><br><span class="line">npm ERR! This is probably not a problem with npm. There is likely additional logging output above.</span><br></pre></td></tr></table></figure><p>解决办法：<br>core-js@2.6.11出现错误<br>执行：npm install core-js@2.6.11 --ignore-scripts</p></li><li><p>run ‘npm audit fix’ to fix them, or ‘npm audit’ for details</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">found 153 vulnerabilities (10 low, 8 moderate, 130 high, 5 critical) </span><br><span class="line">run `npm audit fix` to fix them, or `npm audit` for details</span><br></pre></td></tr></table></figure><p>如果 npm audit fix，之后还是有问题，可能是不能自动 fix，可以先 npm update，然后 npm audit 查看详情，再手动安装相关库。</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>离散数学 1.1-1.5</title>
      <link href="/2020/02/29/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6-1-1-1-5/"/>
      <url>/2020/02/29/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6-1-1-1-5/</url>
      
        <content type="html"><![CDATA[<h2 id="命题">命题</h2><p>命题：是用陈述句表示的一个或者为真或者为假，但不能同时既为真又为假的判断语句。（或判断结果唯一的陈述句，或客观上存在唯一真值的陈述句）<br>判断下列句子中那些是命题？若是命题的，判断其真值。</p><table><thead><tr><th style="text-align:left">句子</th><th style="text-align:left">判断</th></tr></thead><tbody><tr><td style="text-align:left">北京是中国的首都。</td><td style="text-align:left">Y真</td></tr><tr><td style="text-align:left">2+3=6。</td><td style="text-align:left">Y假</td></tr><tr><td style="text-align:left">3-x=5。</td><td style="text-align:left">N真值不确定</td></tr><tr><td style="text-align:left">请关上门。</td><td style="text-align:left">N祈使句</td></tr><tr><td style="text-align:left">几点了？</td><td style="text-align:left">N疑问句</td></tr><tr><td style="text-align:left">除地球外的星球有生物。</td><td style="text-align:left">Y真值确定，但未知</td></tr><tr><td style="text-align:left">多漂亮的花啊！</td><td style="text-align:left">N感叹句</td></tr><tr><td style="text-align:left">我只给所有不给自己理发的人理发。</td><td style="text-align:left">悖论</td></tr></tbody></table><p>引入英文字母表示任意的命题：表示命题的符号称为命题变量，通常用p、q、r…表示命题变量。命题变量没有真值，只有表示一个确定的命题后，才有真值。</p><h2 id="命题公式">命题公式</h2><p>命题常元：代表特定简单命题<br>命题变元：代表任意命题，取值1（真）或0（假）的变量</p><ul><li><p>定义命题公式（公式）的定义如下：<br>1、每一个命题常元或命题变元都是命题公式。<br>2、如果A是命题公式，则（¬A）是命题公式。<br>3、如果A和B都是命题公式，则（A∧B），（A∨B），（A→B），（A↔B）<br>都是命题公式。<br>4、一个由命题常元或命题变元、联结词和括号所组成的符号串是命题公式，当且仅当这个符号串是有限次应用上面的步骤得到的。<br>● 一个含有命题变元的命题公式的真值是不确定的。<br>● 只有当公式中的所有命题变元被指定代表特定的命题时，命题公式才成为命题，其真值才唯一确定。</p></li><li><p>赋值定义：若命题公式A含有的全部命题变元为$p_1$,$p_2$…$p_n$，给$p_1$,$p_2$…$p_n$指定一组真值，称为对A的一个解释或赋值。使A的真值为真的赋值称为成真赋值，使A的真值为假的赋值称为成假赋值。</p></li><li><p>真值表：命题公式在所有可能的赋值下的取值的列表含n个变项的公式有$2^n$个赋值。</p></li><li><p>命题公式的分类：</p><ol><li>若A在它的各种赋值下取值均为真，则称A为<code>重言式</code>或<code>永真式</code>。</li><li>若A在它的各种赋值下取值均为假，则称A为<code>矛盾式</code>或<code>永假式</code>。</li><li>若至少存在一种赋值使A的真值为真，则称A为<code>可满足式</code>。【重言式也是一种可满足式】</li></ol></li></ul><h2 id="等值演算">等值演算</h2><ul><li>等价关系式：设A和B是两个命题（或命题公式），若A$\leftrightarrow$B是永真式，命题A和B称为逻辑等价的，可记为A$\Leftrightarrow$B。<br>说明：A$\Leftrightarrow$B是永真式，表示命题公式A和B在所有的赋值下都有相同的真值，也就是说命题公式A和B有相同的真值表。【可以用真值表来判断两个命题是否等价】，例如p$\rightarrow$q和¬p$\vee$q真值表相同，故p$\rightarrow$q$\Leftrightarrow$¬p$\vee$q。<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/sp20200301_170639_277.png" alt><br>另外还有一些常用的等值式模式：<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/sp20200301_171109_604.png" width="334" align="left"><br>（7）p$\leftrightarrow$q$\Leftrightarrow$(p$\rightarrow$q)$\wedge$(q$\rightarrow$p)等价等值式<br>（8）p$\leftrightarrow$q$\Leftrightarrow$¬p$\leftrightarrow$¬q等价否定等值式<br>（9）(p$\rightarrow$q)$\wedge$(p$\rightarrow$¬q)$\Leftrightarrow$¬p归谬论</li><li>置换规则：若公式G中的一部分A（包含G中几个连续的符号）是公式，称A为G的子公式；用与A逻辑等价的公式B置换A不改变公式G的真值。<br>利用已知的等价关系式，将其中的子公式用和它等价的公式置换可以推出其它一些等价关系式，这一过程称为<font color="red">命题的等价运算</font>。<br>利用命题的等价运算，可以<code>判断两个命题是否等价</code>、<code>判断命题公式的类型</code>、<code>命题公式的化简【逻辑电路的化简】</code>等</li><li>原命题、逆命题、否命题、逆否命题之间的关系<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/sp20200301_182653_199.png" width="361"></li></ul><h2 id="联结词">联结词</h2><ul><li>常用二元联结词<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/sp20200301_183416_062.png" alt></li></ul><h2 id="范式">范式</h2><ul><li>原子公式和原子公式的否定统称为文字。如果一个文字恰为另一个文字的否定，则称它们为相反的文字。（注意¬p不是文字）</li><li>设n是正整数，$A_1…,A_n$。都是文字，称$A_1$$\vee$…$\vee$$A_n$为简单析取式，称$A_1$$\wedge$…$\wedge$$A_n$为简单合取式。</li><li>设n是正整数，若$A_1…,A_n$都是简单合取式，则称$A_1$$\vee$…$\vee$$A_n$为<font color="red">析取范式</font>；若$A_1…,A_n$。都是简单析取式，则称$A_1$$\wedge$…$\wedge$$A_n$为<font color="red">合取范式</font>。</li><li>范式存在定理：任何一个命题公式都存在着与之等值的析取范式与合取范式。</li><li>公式的析取范式与合取范式不唯一<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/Snipaste_2020-03-01_19-01-11.png" width="756" align="left"></li><li>极大极小项：设n是正整数，$p_1,…,p_n$。是不同的命题变元.若对于每个i≤n，$A_i$是$p_i$或¬p，则称$A_1$$\vee$…$\vee$$A_n$为关于$p_1,…,p_n$的极大项，$A_1$$\wedge$…$\wedge$$A_n$为关于$p_1,…,p_n$的极小项。在极大项和极小项中不允许出现相反的文字，也不允许一个文字出现多次。<br>每个极小项有唯一的使其为真的真值赋值，称这个真值赋值为该极小项对应的真值赋值。每个极大项有唯一的使其为假的真值赋值，称这个真值赋值为该极大项对应的真值赋值。这样，可以在极小项（极大项）和真值赋值之间建立一一对应。<br>习惯上，把0看做0个公式的析取，把1看做0个公式的合取.因此，0可看做关于ε的极大项，1可看做关于ε的极小项，其中ε为0个命题变元的序列。</li><li>定理：任何命题公式的主析(合)取范式都是存在且唯一的。<br>设m是正整数.若$A_1…,A_m$是关于$p_1…,p_n$的不同极小项，则称$A_1$$\vee$…$\vee$$A_m$为关于$p_1…,p_n$的主析取范式.若$A_1…,A_m$是关于$p_1…,p_n$的不同极大项，则称$A_1$$\vee$…$\vee$$A_m$为关于$p_1…,p_n$的主合取范式。主析取范式和主合取范式统称为主范式.<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/Snipaste_2020-03-01_22-26-33.png" alt></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>毕业设计简介</title>
      <link href="/2020/02/21/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/"/>
      <url>/2020/02/21/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="毕业设计简介">毕业设计简介</h2><ul><li>课题内容：<br>课题以移动端二手商品信息共享平台为主题，设计并实现一个基于Android的在线校园实时二手商品信息共享平台。平台可向用户提供各类二手商品信息，并可上传用户商品信息以供他人查看，用户亦可就某样商品向卖家咨询。平台具有五大基本功能：<br>① 登录注册：允许用户通过手机号、密码进行注册及登录。<br>② 商品信息管理：允许用户发布商品、查看商品、修改商品、下架商品、重新发布商品及删除商品。<br>③ 商品社区：允许用户浏览商品、点赞商品、收藏商品、评论商品。<br>④ 好友管理：允许用户关注好友、取消关注好友、查看粉丝、查看好友。<br>⑤ 即时通讯：允许用户和卖家就二手商品问题进行沟通与交流。</li><li>客户端具体功能如下图所示：<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221162723.png" alt="Android客户端功能模块设计图"></li></ul><!--* 平台网络结构  三台服务器分别为：①腾讯云服务器，该服务器为Centos 7.5系统，已搭建Java 1.8、Tomcat 8.5、Mysql 5.7运行环境。主要用于处理平台基本业务并储存平台用户及商品数据信息。②阿里云对象储存服务器，该服务器主要用于存储图片文件。③融云服务器，该服务器用于实现Android移动客户端间的即时通讯及推送功能。<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221163732.png" width="70%">--><ul><li><p>服务器端框架集成<br>服务器端采用SSM（Spring+SpringMVC+MyBatis）作为整个后端框架，其层次结构如下图所示。其中表现层负责接收用户请求、转发请求、显示数据等，业务层主要负责组织业务逻辑，持久层负责持久化业务对象，而Spring作为“粘合剂”将各层进行整合<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221172325.png" width="191"></p></li><li><p>API接口设计——Token认证<br>设计API接口共计23个，覆盖登录、注册、更新用户信息、关注好友、发布商品、收藏商品、点赞商品等接口。为防止接口被恶意调用，除登录注册接口外均需进行Token用户身份信息验证。用户身份信息验证流程具体如下所示：<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221164303.png" width="520" alt="Token用户信息验证机制时序图"></p></li><li><p>毕设项目部分截图如下<br><br><img src="https://ae01.alicdn.com/kf/Hf2a386adbc234d46ad1263286ac80d843.jpg"></p></li></ul><h2 id="毕设工作量">毕设工作量</h2><ul><li>本次课设使用Android Studio编写Android原生应用程序，使用IntelliJ IDEA编写后端API及Web端后台管理，使用PostMan测试API功能正确性。其中客户端代码量近<font color="red">16118</font>行，服务器端代码量近<font color="red">3124</font>行，guns后台管理代码量近<font color="red">800</font>行。</li><li>在编码过程中，本次亦使用部分第三方插件、库加速项目构建，如使用Lombok让JavaBean类在编译时自动生成getter、setter方法，而无需手动编写。如使用ButterKnife以注解方式绑定控件减少大量findViewById及setOnClickListener代码。如使用MVPHelper插件快速生成MVP模式所需的接口及类。如使用EventBus将事件发送者和接收者分离，实现Fragment、Activity间的数据传递，大大简化组件间通信。</li><li>本次课设并非一帆风顺，客户端git提交版本数<font color="red">107</font>，服务器端git提交版本数<font color="red">46</font>，其中不少提交均为纠正上一版本错误，例如客户端向服务器端请求后出现的空指针异常、数据库无法插入emoji表情以及更新用户信息却未缓存这样的低级错误。</li><li>最后，感谢我的导师，没有导师每周耐心、细心的教导，本次课设亦无法及时、高效地完成。在此衷心感谢我的导师，愿老师身体健康，工作顺心！</li></ul><h2 id="毕设获奖情况">毕设获奖情况</h2><ul><li>获2019年北京市普通高等学校优秀本科毕业设计（论文）</li></ul><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221214151.png" width="580"><ul><li>获2018年度北京高等学校高水平人才交叉培养“实培计划”成果认定</li></ul><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221214618.png" width="650">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>如何确定一个相对合适的学习率</title>
      <link href="/2019/05/01/%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
      <url>/2019/05/01/%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E5%AD%A6%E4%B9%A0%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h2 id="为何要寻找合适的学习率lr">为何要寻找合适的学习率lr</h2><ul><li>经过了大量炼丹的同学都知道，超参数是一个非常玄乎的东西，比如batch size，学习率等，论文中设定的超参数一般都是靠经验决定的。但是超参数往往又特别重要，比如学习率，如果设置了一个太大的学习率，那么loss就爆了，设置的学习率太小，需要等待的时间就特别长，那么我们是否有一个相对合理的办法来决定我们的初始学习率呢？<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200301014342.png" width="425"></li></ul><h2 id="如何寻找？">如何寻找？</h2><ul><li>文章 “How Do You Find A Good Learning Rate” 提供了一个相对可靠的寻找方法。即在训练的过程中从小到大不断增加学习率，并记录每一轮学习率对应的loss信息，从而绘制出一张loss、学习率关系曲线图。<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/1900456b9cd6de1946e0d9a250f333da_1556722410978x5gf2f93.png" alt="Loss、Lr曲线图"><br>通过这张图，我们可以大致确定学习率的范围。</li></ul><h2 id="实践">实践</h2><ul><li><p>光说不练假把式，这次，我们使用Pytorch(1.1版本)用于图像识别，图像数据CIFAR10。实践代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">   [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                       download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">8</span>,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                      download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">       super(Net, self).__init__()</span><br><span class="line">       self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">18</span>, <span class="number">5</span>)</span><br><span class="line">       self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">       self.conv2 = nn.Conv2d(<span class="number">18</span>, <span class="number">32</span>, <span class="number">5</span>)</span><br><span class="line">       self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">       self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">       self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">       x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">       x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">       x = x.view(<span class="number">-1</span>, <span class="number">32</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">       x = F.relu(self.fc1(x))</span><br><span class="line">       x = F.relu(self.fc2(x))</span><br><span class="line">       x = self.fc3(x)</span><br><span class="line">       <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lr</span><span class="params">(init_value = <span class="number">1e-5</span>, final_value=<span class="number">1e-1</span>, beta = <span class="number">0.98</span>)</span>:</span></span><br><span class="line">    num = len(trainloader)<span class="number">-1</span></span><br><span class="line">    mult = (final_value / init_value) ** (<span class="number">1</span>/num)</span><br><span class="line">    lr = init_value</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line">    avg_loss = <span class="number">0.</span></span><br><span class="line">    best_loss = <span class="number">0.</span></span><br><span class="line">    batch_num = <span class="number">0</span></span><br><span class="line">    losses = []</span><br><span class="line">    log_lrs = []</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> trainloader:</span><br><span class="line">        batch_num += <span class="number">1</span></span><br><span class="line">        <span class="comment">#As before, get the loss for this mini-batch of inputs/outputs</span></span><br><span class="line">        inputs,labels = data</span><br><span class="line">        inputs, labels = inputs.to(device),labels.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment">#Compute the smoothed loss</span></span><br><span class="line">        avg_loss = beta * avg_loss + (<span class="number">1</span>-beta) *loss.data.item()</span><br><span class="line">        smoothed_loss = avg_loss / (<span class="number">1</span> - beta**batch_num)</span><br><span class="line">        <span class="comment">#Stop if the loss is exploding</span></span><br><span class="line">        <span class="keyword">if</span> batch_num &gt; <span class="number">1</span> <span class="keyword">and</span> smoothed_loss &gt; <span class="number">4</span> * best_loss:</span><br><span class="line">            <span class="keyword">return</span> log_lrs, losses</span><br><span class="line">        <span class="comment">#Record the best loss</span></span><br><span class="line">        <span class="keyword">if</span> smoothed_loss &lt; best_loss <span class="keyword">or</span> batch_num==<span class="number">1</span>:</span><br><span class="line">            best_loss = smoothed_loss</span><br><span class="line">        <span class="comment">#Store the values</span></span><br><span class="line">        losses.append(smoothed_loss)</span><br><span class="line">        log_lrs.append(math.log10(lr))</span><br><span class="line">        print(<span class="string">"losses"</span>,smoothed_loss,<span class="string">"  : log_lrs"</span>,math.log10(lr))</span><br><span class="line">        <span class="comment">#Do the SGD step</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment">#Update the lr for the next step</span></span><br><span class="line">        lr *= mult</span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line">    <span class="keyword">return</span> log_lrs, losses</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    net = Net().to(device)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.Adam(net.parameters(), lr=<span class="number">6.30957e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -------------寻找学习率------------------</span></span><br><span class="line">    log_lrs,losses = find_lr()</span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.plot(np.array(log_lrs), np.array(losses), <span class="string">'r-'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="string">''' --- 注释部分为训练部分 找到学习率后这段取消注释----</span></span><br><span class="line"><span class="string">    for epoch in range(2):  # 多批次循环</span></span><br><span class="line"><span class="string">        running_loss = 0.0</span></span><br><span class="line"><span class="string">        for i, data in enumerate(trainloader, 0):</span></span><br><span class="line"><span class="string">            # 获取输入</span></span><br><span class="line"><span class="string">            inputs, labels = data</span></span><br><span class="line"><span class="string">            inputs, labels = inputs.to(device),labels.to(device)</span></span><br><span class="line"><span class="string">            # 梯度置0</span></span><br><span class="line"><span class="string">            optimizer.zero_grad()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            # 正向传播，反向传播，优化</span></span><br><span class="line"><span class="string">            outputs = net(inputs)</span></span><br><span class="line"><span class="string">            loss = criterion(outputs, labels)</span></span><br><span class="line"><span class="string">            loss.backward()</span></span><br><span class="line"><span class="string">            optimizer.step()</span></span><br><span class="line"><span class="string">            # 打印状态信息</span></span><br><span class="line"><span class="string">            running_loss += loss.item()</span></span><br><span class="line"><span class="string">            if i % 100 == 99:    # 每2000批次打印一次</span></span><br><span class="line"><span class="string">                print('[%d, %5d] loss: %.3f' %</span></span><br><span class="line"><span class="string">                      (epoch + 1, i + 1, running_loss / 100))</span></span><br><span class="line"><span class="string">                running_loss = 0.0</span></span><br><span class="line"><span class="string">    print('Finished Training')</span></span><br><span class="line"><span class="string">    dataiter = iter(testloader)</span></span><br><span class="line"><span class="string">    images, labels = dataiter.next()</span></span><br><span class="line"><span class="string">    # 显示图片</span></span><br><span class="line"><span class="string">    imshow(torchvision.utils.make_grid(images))</span></span><br><span class="line"><span class="string">    images, labels = images.to(device), labels.to(device)</span></span><br><span class="line"><span class="string">    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))</span></span><br><span class="line"><span class="string">    outputs = net(images)</span></span><br><span class="line"><span class="string">    _, predicted = torch.max(outputs, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]</span></span><br><span class="line"><span class="string">                                  for j in range(4)))</span></span><br><span class="line"><span class="string">    correct = 0</span></span><br><span class="line"><span class="string">    total = 0</span></span><br><span class="line"><span class="string">    with torch.no_grad():</span></span><br><span class="line"><span class="string">        for data in testloader:</span></span><br><span class="line"><span class="string">            images, labels = data</span></span><br><span class="line"><span class="string">            images, labels = images.to(device),labels.to(device)</span></span><br><span class="line"><span class="string">            outputs = net(images)</span></span><br><span class="line"><span class="string">            _, predicted = torch.max(outputs.data, 1)</span></span><br><span class="line"><span class="string">            total += labels.size(0)</span></span><br><span class="line"><span class="string">            correct += (predicted == labels).sum().item()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print('Accuracy of the net![paste image](http://pqijd5ur8.bkt.clouddn.com/1556723319786h9drvq7w.png)work on the 10000 test images: %d %%' % (</span></span><br><span class="line"><span class="string">            100 * correct / total))</span></span><br><span class="line"><span class="string">    # '''</span></span><br></pre></td></tr></table></figure><p>运行代码，我们可以得到这么一张图片：<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/f8d3810d65d0060a82324099184da3b1_1556723354833hk6ypeif.png" alt="Loss、Lr关系曲线图"><br>从图像上来看，合适的学习率大致在1e-4.5、1e-3之间，采几个点简单测试（仅训练2轮），结果如下：</p><table><thead><tr><th style="text-align:center">学习率</th><th style="text-align:center">最终Loss</th><th style="text-align:center">测试集准确率</th></tr></thead><tbody><tr><td style="text-align:center">$10^{-4.5}$</td><td style="text-align:center">1.41</td><td style="text-align:center">48%</td></tr><tr><td style="text-align:center">$10^{-4}$</td><td style="text-align:center">1.29</td><td style="text-align:center">56%</td></tr><tr><td style="text-align:center">$10^{-3}$</td><td style="text-align:center">1.193</td><td style="text-align:center">59%</td></tr><tr><td style="text-align:center">$10^{-3.2}$</td><td style="text-align:center">1.016</td><td style="text-align:center">62%</td></tr></tbody></table></li><li><p>当$lr = 10^{-3.2}$时，训练效果看上去还不错，但是随着训练的次数不断增加，loss不断降低，之前的学习率对于后面的训练来讲可能过大，从而导致Loss不断震荡而迟迟无法达到预期。本次进行两组实验，一组$lr=10^{-3.2}$，一组采用动态的$lr=10^{((1.4673913* loss)^{(1/3)}-4.5)}$。动态lr的指数与loss的关系如下图所示【Loss下降，lr 10的指数随着下降】：<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221134650.png" alt><br>该图主要依据我们之前获得的Loss、Lr关系曲线图确定</p></li><li><p>测试结果如下：<br>动态Lr前期效果与$Lr=10^{-3.2}$相当，均可使得Loss迅速下降，但Loss降至0.25后，此时$10^{-3.2}$的学习率相对较大，使得Loss不断震荡难以下降，而动态Lr下Loss则表现相对平稳并持续下降。<br><img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/8aea6d2591ac377ab2a8d55bb06de7c8_15567821029676mc8ef0b.png" alt></p></li></ul><p><a href="https://blog.csdn.net/briblue/article/details/84325722" target="_blank" rel="noopener">https://blog.csdn.net/briblue/article/details/84325722</a><br><a href="https://blog.csdn.net/tonydz0523/article/details/79073146" target="_blank" rel="noopener">https://blog.csdn.net/tonydz0523/article/details/79073146</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习环境GPU-Cuda、cudnn下载</title>
      <link href="/2019/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83GPU-Cuda%E3%80%81cudnn%E4%B8%8B%E8%BD%BD/"/>
      <url>/2019/04/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83GPU-Cuda%E3%80%81cudnn%E4%B8%8B%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h2 id="Tensorflow-ImportError-DLL-load-failed-找不到指定的模块。">Tensorflow ImportError: DLL load failed: 找不到指定的模块。</h2><h4 id="问题：升级了Tensorflow，没有升级CUDA®-Toolkit，cuDNN。不同版本的Tensorflow对应不同的CUDA、cuDNN。">问题：升级了Tensorflow，没有升级CUDA® Toolkit，cuDNN。不同版本的Tensorflow对应不同的CUDA、cuDNN。</h4><p> <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">各版本Cuda下载</a>  <a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">各版本cuDNN下载</a></p><h3 id="注意：不要用迅雷下载，除非你想这样">注意：不要用迅雷下载，除非你想这样</h3><ul><li><img align="left" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/7437d61e1a863b4ebded2fb552933ce3_Snipaste_2019-04-26_16-04-55.png">  </li><li>下载方法：使用百度云离线下载，先保存到百度云，然后下载</li></ul>]]></content>
      
      
      <categories>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensorflow踩坑 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小车驱动</title>
      <link href="/2019/04/25/car_driver/"/>
      <url>/2019/04/25/car_driver/</url>
      
        <content type="html"><![CDATA[<h2 id="小车电机、舵机硬件信息">小车电机、舵机硬件信息</h2><h3 id="直流有刷电机调速与换向控制器APO-B2">直流有刷电机调速与换向控制器APO-B2</h3><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/Snipaste_2020-02-23_22-18-08.jpg">  <h4 id="电机控制器的三种控制方式">电机控制器的三种控制方式</h4><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/APO-B2-Use-First-Way.png"><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/APO-B2-Use-Second-Way.png"><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/APO-B2-Use-Third-Way.png">### 大功率直线舵机控制器 ASMT-01 series<img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/ASMT-01-0.png">#### 舵机控制器的三种控制方式<img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/ASMT-01-Use-First-Way.png"><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/ASMT-01-Use-Second-Way.png"><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/ASMT-01-Use-Third-Way.png"><h2 id="电机、舵机控制器连线">电机、舵机控制器连线</h2><p>小车电机及舵机均通过树莓派进行控制，电机PWM模式，舵机PPM模式(“2ms”脉冲宽度模式)。</p><h3 id="电机、树莓派连线">电机、树莓派连线</h3><img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/2019-2-21-raspberry-1.png">### 舵机、树莓派连线<img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/2019-2-21-raspberry-2.png">## 小车前进后退函数### 预备知识：* 占空比数值范围：0-100，占空比越高，电机转速越快。* 另外电位高低决定电机旋转的方向。<img align="left" width="90%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/2019-2-21-raspberry-3.png"><img align="left" width="60%" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/duty-car.png"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 小车前进函数 duty_cycle即占空比（0-100）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">car_forward</span><span class="params">(duty_cycle)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'car_forward'</span></span><br><span class="line">    p.start(duty_cycle)</span><br><span class="line">    GPIO.output(DR, GPIO.LOW)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 小车后退函数，与car_forward(duty_cycle)相比只是改变了IO口电位高低值，用法与前者一致</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">car_back</span><span class="params">(duty_cycle)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'car_back'</span></span><br><span class="line">    p.start(duty_cycle)</span><br><span class="line">    GPIO.output(DR, GPIO.HIGH)</span><br></pre></td></tr></table></figure><h2 id="小车方向函数">小车方向函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#car_direction用于设置小车方向</span></span><br><span class="line"><span class="comment">#pulse_width数值范围为0-15，pulse_width为7.5时，小车直行</span></span><br><span class="line"><span class="comment">#pulse_width&lt;7.5时，小车左转，pulse_width&gt;7.5时，小车右转。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">car_direction</span><span class="params">(pulse_width)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> pulse_width</span><br><span class="line">    d.start(pulse_width)</span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h2 id="小车电机、舵机详细文档">小车电机、舵机详细文档</h2><p>文档点击下载：<a href="%5Cdocuments%5CAPO-B2_CN.pdf">电机文档</a>、<a href="%5Cdocuments%5CASMT-01_series_CN.pdf">舵机</a></p>]]></content>
      
      
      <categories>
          
          <category> 无人驾驶小车 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小车硬件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow学习教程推荐</title>
      <link href="/2018/04/26/Tensorflow%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%E6%8E%A8%E8%8D%90/"/>
      <url>/2018/04/26/Tensorflow%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<h2 id="教程推荐">教程推荐</h2><h3 id="TensorFlow-Course">TensorFlow-Course</h3><p>该<a href="https://github.com/machinelearningmindset/TensorFlow-Course#why-use-tensorflow" target="_blank" rel="noopener">教程</a>在GitHub超过11,358星，主要包含三个部分（Basics、Basic Machine Learning、Neural Networks），每一部分均给出源代码及其详细参考文档。</p><ul><li>TensorFlow的基础知识<br><img align="left" width="550" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221140558.png"></li><li>线性回归模型等基本的机器学习方法<br><img align="left" width="550" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/36fead2e0f97ebf5b32386909f16226f_15562816390968cx4jdki.png"></li><li>神经网络的基本教程及代码。<br><img align="left" width="600" src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221140641.png"><br><a href="https://github.com/machinelearningmindset/TensorFlow-Course#why-use-tensorflow" target="_blank" rel="noopener">TensorFlow-Course地址   Go！</a></li></ul><h3 id="TensorFlow-Examples">TensorFlow-Examples</h3><ul><li>项目介绍：TensorFlow-Examples旨在通过示例轻松地深入研究TensorFlow。为了可读性，它包括了notebooks和source codes的解释，为两个TF v1和v2。它适合于那些想要找到清晰、简洁例子的初学者们。除了传统的“原始”TensorFlow实现之外，还可以找到最新的TensorFlow API实践(例如layers, estimator, dataset,…)<br></li><li>推荐的主要原因：例子简单清晰、代码紧跟Tensorflow进行迭代、对于初学者较友好</li></ul>]]></content>
      
      
      <categories>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习实战 gym学习及二次开发</title>
      <link href="/2018/01/01/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-gym%E5%AD%A6%E4%B9%A0%E5%8F%8A%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91/"/>
      <url>/2018/01/01/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-gym%E5%AD%A6%E4%B9%A0%E5%8F%8A%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<p>该学习笔记基于知乎郭宪老师专栏 <a href="https://zhuanlan.zhihu.com/sharerl" target="_blank" rel="noopener">《强化学习知识大讲堂》</a></p><h2 id="gym介绍：">gym介绍：</h2><p>gym并非新东西，它是用python语言编写的仿真器，其可与谷歌的Tensorflow联合使用。当前gym只支持Linux and OS X，暂不支持Windows系统。故接下来的操作均在ubuntu环境下执行。</p><h3 id="ubuntu下安装anaconda：">ubuntu下安装anaconda：</h3><ul><li>Step1：为了便于管理，需先装 <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive" target="_blank" rel="noopener">anaconda</a>（我装的是Anaconda3-4.3.0）</li><li>Step2：安装anaconda。anaconda下载完成后，安装包会在Dowloads文件夹下，在终端（Ctrl+Alt+T打开终端）键入cd Downloads， 然后键入 bash Anaconda3_4.3.0-Linux-x86_64.sh（小技巧，键入bash an然后按Tab键，linux系统会自动补全后面的参数）</li><li>Step3：安装过程会询问是否将路径安装到环境变量中，键入yes， 至此Anaconda安装完成。你会在目录/home/你的用户名文件夹下面看到anaconda3。</li></ul><h3 id="利用anaconda建一个虚拟环境（可跳过）：">利用anaconda建一个虚拟环境（可跳过）：</h3><ul><li>Anaconda创建虚拟环境的格式为：conda create -n 你要创建的名字 python=版本号。比如我创建的虚拟环境名字为gymlab(你可以用自己的环境名）, 用的python版本号为3.5，可这样写：</li></ul><center>conda create -n gymlab python=3.5</center>* 操作完后，会在anaconda3/envs文件夹下多一个gymlab。Python3.5就在gymlab下得lib文件夹中。  虚拟环境的优点在于相互间不会出现干扰，故在虚拟环境下安装使用gym不会影响其他环境。一旦环境出现问题可直接删除再创新环境。<h3 id="安装gym">安装gym</h3><ul><li>Step1. 键入git clone openai/gym的地址，将gym克隆到计算机中. 如果你的计算机中没有安装git, 那么可以先安装git：sudo apt install git</li><li>Step2. cd gym 进入gym文件夹</li><li>Step3. 【假如有虚拟环境，则在虚拟环境中安装】pip install -e '.[all]'进行完全安装。装完后可将gym安装目录写到环境变量中，一种方法是打开.bashrc文件，在末尾加入语句：export PYTHONPATH=你的gym目录：$PYTHONPATH。<br><font color="red">对于step3, 如果报错则安装依赖项，键入命令sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig，安装时间可能很长，然后再按照Step3的命令安装。<br>注意：先sudo pip install -e ‘.[all]’ 假如有错再安装依赖。pip不要自己安装，直接用anaconda的。假如创建了虚拟环境，可在虚拟环境下安装使用。&lt;/font color=red&gt;</font></li><li>Step4.  测试Gym是否安装成功。下面提供一个简单的测试用例：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> 开一个终端(ctr+alt+t)， 然后激活用anaconda建立的虚拟环境：</span><br><span class="line">source activate gymlab</span><br><span class="line"><span class="number">2.</span> 运行python:</span><br><span class="line">python</span><br><span class="line"><span class="number">3.</span> 导入gym模块</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="number">4.</span> 创建一个小车倒立摆模型</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="number">5.</span> 初始化环境</span><br><span class="line">env.reset()</span><br><span class="line"><span class="number">6.</span> 刷新当前环境，并显示</span><br><span class="line">env.render()</span><br></pre></td></tr></table></figure><ul><li>假如gym安装成功，我们将看到小车倒立摆模型.如下图所示：<img src="https://gitee.com/xiongjun131/blogImage/raw/master/img/20200221152343.png" alt></li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
